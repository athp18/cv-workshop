{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/athp18/cv-workshop/blob/main/DataTeach_CV_Workshop_10_30_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Computer Vision\n",
        "\n",
        "## Goal of this notebook\n",
        "\n",
        "We want to learn some of the basics computer visio, building up from basic image processing to running inference with a deep learning model! We'll start off with some basic operations using OpenCV and build up to running a deep learning model."
      ],
      "metadata": {
        "id": "PCuoi792kEh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A few basic libraries:\n",
        "\n",
        "- **PyTorch**: A Python library that offers support for creating deep learning models\n",
        "- **OpenCV**: A computer vision library that offers support for a wide range of image processing algorithms\n",
        "- **NumPy**: A library for efficient linear algebra and computation\n",
        "- **PIL (Python Imaging Library)**: A library for image processing (used by PyTorch)\n",
        "- **os**: A library for basic operating system commands\n",
        "- **Matplotlib**: A library for displaying and plotting"
      ],
      "metadata": {
        "id": "Qmt2gI9ZkkQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we get started, let's download a few images and datasets to make things easier for ourselves."
      ],
      "metadata": {
        "id": "dJeTpNDu4UFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://www.dropbox.com/scl/fi/rxkkl5ypkulvk8u1s2rm0/imgs.zip?rlkey=c84rxn3xw84livtdw1rxay66q&st=l5frpa9h&dl=1\" -O images.zip\n",
        "!unzip images.zip -x \"__MACOSX/*\" \"*.DS_Store\""
      ],
      "metadata": {
        "id": "GpnVMOwy4YWG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also make sure that you're connected to a GPU Runtime (you can switch in the top right)."
      ],
      "metadata": {
        "id": "iE7KcKQcw2Kd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also mount our Google Drive to this notebook. Any time you disconnect from Colab, any files created get deleted, so this is a good way to keep a copy."
      ],
      "metadata": {
        "id": "iw_Ms9futSgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.drive import mount\n",
        "\n",
        "mount('/content/drive')"
      ],
      "metadata": {
        "id": "FzAuTw7Bf8sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Matrices\n",
        "\n",
        "- The foundation of machine learning and computer vision is a **matrix**.\n",
        "\n",
        "![Image of Matrix](https://dcvp84mxptlac.cloudfront.net/diagrams2/equation-1-example-of-a-matrix.png)\n",
        "\n",
        "A matrix is just a way to organize numbers. For example, mathematically, say we have two equations: x + y = 12 and 2x + 3y = 26. We can organize the coefficients and constants as a 2x3 matrix, like this:\n",
        "\n",
        "![matrix_image.png](data:image/png;base64,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)\n",
        "\n",
        "An image too can be represented as a matrix. A few examples to show this below:\n",
        "\n"
      ],
      "metadata": {
        "id": "70acRCw6k-RF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The matrix above can be represented in Python in several ways:\n",
        "\n",
        "```\n",
        "- numpy.array([1,1,12],[2,3,26])\n",
        "- torch.tensor([1,1,12], [2,3,26])\n",
        "- [[1,1,12], [2,3,26]]\n",
        "```\n",
        "The first is a Numpy array (or the foundation for OpenCV, which we'll get to soon). The second is a PyTorch tensor (the foundation for PyTorch), which have been optimized for GPU operations. And the third is a regular Python list -- what you'll probably use out of most data science contexts :)"
      ],
      "metadata": {
        "id": "4RYD7OqeH5lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "image = cv2.imread('./images/panda.jpg')\n",
        "image_raw = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "image_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "plt.imshow(image_gray)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BhWRpe9G3yYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_gray)"
      ],
      "metadata": {
        "id": "lOz91jzo8rYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(image_raw[:3]) # truncate so we don't show everything"
      ],
      "metadata": {
        "id": "R4LWnCGD9La_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of the raw image\", image_raw.shape)\n",
        "print(\"Shape of the grayscale image\", image_gray.shape)"
      ],
      "metadata": {
        "id": "SEedJ8mn9Q9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can we see from this? In a grayscale image, each entry of the matrix is represented by a single value indicating how \"gray\" the image is. In a regular, color image, each entry has 3 values, corresponding to red, green, and blue. Each value has a range from 0 to 255, corresponding to how much color is in the image."
      ],
      "metadata": {
        "id": "LyM7CwZc9aPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# So! Let's do it ourselves!\n",
        "\n",
        "(Make sure to run the hidden cell below or else it will throw you an error)"
      ],
      "metadata": {
        "id": "zNZNqrk3_-UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions\n",
        "# Function to add a subplot with relative size visualization\n",
        "def add_subplot_with_relative_size(position, img, title):\n",
        "    ax = fig.add_subplot(gs[position])\n",
        "\n",
        "    # Create a background to show original image dimensions\n",
        "    background = np.zeros((height, width, 3), dtype=np.uint8) + 240  # Light gray background\n",
        "\n",
        "    # Calculate position to center the transformed image\n",
        "    img_h, img_w = img.shape[:2]\n",
        "    y_offset = (height - img_h) // 2\n",
        "    x_offset = (width - img_w) // 2\n",
        "\n",
        "    # Place the transformed image on the background\n",
        "    if img_h <= height and img_w <= width:\n",
        "        background[y_offset:y_offset+img_h, x_offset:x_offset+img_w] = img\n",
        "    else:\n",
        "        # For rotated image that might exceed bounds, crop it\n",
        "        visible_h = min(img_h, height)\n",
        "        visible_w = min(img_w, width)\n",
        "        background = img[:visible_h, :visible_w]\n",
        "\n",
        "    ax.imshow(background)\n",
        "    ax.set_title(title)\n",
        "\n",
        "    # Add dotted lines to show original dimensions\n",
        "    ax.axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axhline(y=height-1, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n",
        "    ax.axvline(x=width-1, color='gray', linestyle=':', alpha=0.5)"
      ],
      "metadata": {
        "id": "xjBa9y47SgsY",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KPtOqnm_wT5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1: Basic OpenCV commands\n",
        "\n",
        "What we want to learn here are the basics of OpenCV image processing -- that is, image loading, image stitching, etc. We also want to see how images are represented in OpenCV."
      ],
      "metadata": {
        "id": "sCImreqE6xF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 1: Loading and printing basic info about an image with OpenCV\n",
        "\n",
        "image_path = './images/bear.jpg'\n",
        "img = cv2.imread(image_path)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "print(f\"Image shape: {img.shape}\")\n",
        "print(f\"Data type: {img.__}\") # what command should we use to find the data type?\n",
        "print(f\"Min value: {img.__()}, Max value: {img.__()}\") # how can we find the minimum of an array?\n",
        "\n",
        "# Now say we want to break an image and apart and stitch it together? How can we do so?\n",
        "red_channel = img[:, :, 0]\n",
        "green_channel = img[:, :, 1]\n",
        "blue_channel = img[:, :, 2]\n",
        "restitched_image = np.stack((red_channel, green_channel, blue_channel), axis=2).astype(np.uint8)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qwhsqREeABGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the plots\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Original image\n",
        "plt.subplot(151)\n",
        "plt.imshow(img)\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Red channel\n",
        "plt.subplot(152)\n",
        "plt.imshow(red_channel)\n",
        "plt.title('Red Channel')\n",
        "plt.axis('off')\n",
        "\n",
        "# Green channel\n",
        "plt.subplot(153)\n",
        "plt.imshow(green_channel)\n",
        "plt.title('Green Channel')\n",
        "plt.axis('off')\n",
        "\n",
        "# Blue channel\n",
        "plt.subplot(154)\n",
        "plt.imshow(blue_channel)\n",
        "plt.title('Blue Channel')\n",
        "plt.axis('off')\n",
        "\n",
        "# Restitched image\n",
        "plt.subplot(155)\n",
        "plt.imshow(restitched_image)\n",
        "plt.title('Restitched Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D4IR-5JG7n8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2: Basic Image Operations\n",
        "\n",
        "What we want to learn here is how to apply basic image operations (resizing, cropping, rotating) using OpenCV"
      ],
      "metadata": {
        "id": "f0w3dSTX7AIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread('./images/fox.jpg') # Read the image\n",
        "\n",
        "# Convert the image from BGR to RGB\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Get image dimensions\n",
        "height, width = image.shape[:2]\n",
        "\n",
        "# 1. Resizing the Image\n",
        "scale_percent = 50  # percent of original size\n",
        "new_width = int(width * scale_percent / 100)\n",
        "new_height = int(height * scale_percent / 100)\n",
        "resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "# 2. Cropping the Image\n",
        "crop_size = min(200, height, width)  # Ensure crop size doesn't exceed image dimensions\n",
        "center_y, center_x = height // 2, width // 2\n",
        "half_crop = crop_size // 2\n",
        "\n",
        "start_y = center_y - half_crop\n",
        "start_x = center_x - half_crop\n",
        "end_y = center_y + half_crop\n",
        "end_x = center_x + half_crop\n",
        "\n",
        "cropped_image = image[start_y:end_y, start_x:end_x]\n",
        "\n",
        "# 3. Rotating the Image\n",
        "# We want to rotate by 45 degrees\n",
        "center = (width // 2, height // 2)\n",
        "rotation_matrix = cv2.getRotationMatrix2D(center, 45, 1.0)\n",
        "rotated_image = cv2.warpAffine(image, rotation_matrix, (width, height))\n",
        "\n",
        "# Create figure\n",
        "fig = plt.figure(figsize=(20, 5))\n",
        "gs = plt.GridSpec(1, 4, figure=fig)\n",
        "images = [\n",
        "    (image, 'Original Image\\n' + f'({width}x{height})'),\n",
        "    (resized_image, f'Resized Image\\n({new_width}x{new_height})'),\n",
        "    (cropped_image, f'Cropped Image\\n({crop_size}x{crop_size})'),\n",
        "    (rotated_image, 'Rotated Image (45Â°)\\n' + f'({width}x{height})')\n",
        "]\n",
        "\n",
        "for idx, (img, title) in enumerate(images):\n",
        "    add_subplot_with_relative_size(idx, img, title)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8e3gZcXrF1nM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load images\n",
        "dog_image = cv2.imread('./images/pup.jpg')\n",
        "dog_image = cv2.cvtColor(dog_image, cv2.COLOR_BGR2RGB)\n",
        "coin_image = cv2.imread('./images/coins.jpg')\n",
        "coin_image_rgb = cv2.cvtColor(coin_image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Dog Image Processing Steps\n",
        "# Step 1: Apply Gaussian Blur\n",
        "dog_gaussian_blur = cv2.GaussianBlur(dog_image, (7, 7), 5)\n",
        "\n",
        "# Step 2: Convert to Grayscale\n",
        "dog_gray = cv2.cvtColor(dog_gaussian_blur, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# Step 3: Apply Canny Edge Detection\n",
        "dog_edges = cv2.Canny(dog_gray, 100, 200)\n",
        "\n",
        "# Step 4: Find and Draw Contours\n",
        "dog_contours, _ = cv2.findContours(dog_edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "dog_image_with_contours = dog_image.copy()\n",
        "cv2.drawContours(dog_image_with_contours, dog_contours, -1, (255, 0, 0), 2)"
      ],
      "metadata": {
        "id": "d82d7pD1tEwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the plots\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(dog_image)\n",
        "plt.title(\"Original Dog Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(dog_gaussian_blur)\n",
        "plt.title(\"Gaussian Blur\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(dog_edges, cmap='gray')\n",
        "plt.title(\"Canny Edge Detection\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(dog_image_with_contours)\n",
        "plt.title(\"Contours on Dog Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yZGNmftR7Wta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Coin Image Processing Steps\n",
        "# Step 1: Create a black background mask for coins\n",
        "coin_gray = cv2.cvtColor(coin_image, cv2.COLOR_BGR2GRAY)\n",
        "_, binary_mask = cv2.threshold(coin_gray, 240, 255, cv2.THRESH_BINARY_INV)\n",
        "black_bg = coin_image_rgb.copy()\n",
        "black_bg[binary_mask == 0] = [0, 0, 0]\n",
        "\n",
        "# Step 2: Grayscale and Gaussian Blur for Contour Detection\n",
        "gray_black_bg = cv2.cvtColor(black_bg, cv2.COLOR_RGB2GRAY)\n",
        "coin_blurred = cv2.GaussianBlur(gray_black_bg, (9, 9), 2)\n",
        "\n",
        "# Step 3: Find Contours\n",
        "coin_contours, _ = cv2.findContours(coin_blurred, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# Filter contours based on area to reduce noise\n",
        "min_contour_area = 1000\n",
        "filtered_contours = [cnt for cnt in coin_contours if cv2.contourArea(cnt) > min_contour_area]\n",
        "coin_count = len(filtered_contours)\n",
        "\n",
        "# Draw Contours on Original and Black Background\n",
        "coin_output_black = black_bg.copy()\n",
        "coin_output_original = coin_image_rgb.copy()\n",
        "for contour in filtered_contours:\n",
        "    cv2.drawContours(coin_output_black, [contour], -1, (0, 255, 0), 2)\n",
        "    cv2.drawContours(coin_output_original, [contour], -1, (0, 255, 0), 2)"
      ],
      "metadata": {
        "id": "LxlAmM827chA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the plots\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(coin_image_rgb)\n",
        "plt.title(\"Original Coin Image\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.imshow(black_bg)\n",
        "plt.title(\"Black Background Mask\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.imshow(coin_output_black)\n",
        "plt.title(\"Contours on Black Background\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.imshow(coin_output_original)\n",
        "plt.title(f\"Contours on Original Image: {coin_count} coins\")\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uW345Qrz7RnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now to implement our CartoonGAN!\n",
        "\n",
        "CartoonGAN is based on a standard Generative Adversarial Network, but instead of just predicting between fake and real images, it also has to distinguish between smoothed cartoon images. As a result of this, CartoonGAN tends to avoid the problem that a lot of GANs trained on cartoons have, where images can appear overly smoothed."
      ],
      "metadata": {
        "id": "tU0EkBkja2v9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we do anything, we need to download and unzip our datasets."
      ],
      "metadata": {
        "id": "VIIRuf4t8XAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O smoothed_cartoons.zip \"https://www.dropbox.com/scl/fi/w2xpvrsqdhya1j1wunpoe/smoothed_cartoons.zip?rlkey=tr4vm7knz27kwfbax3di5z5fq&st=hgyfzlar&dl=1\"\n",
        "!wget -O cocoimg.zip \"https://www.dropbox.com/scl/fi/oh0lsqhlf6kdbcrglhewf/coco_images.zip?rlkey=ltrlb1fnemfxhvixzcsnbbbaf&st=3vf59pp4&dl=1\"\n",
        "!wget -O cartoons.zip \"https://www.dropbox.com/scl/fi/pcjnxskeju656k6kmkgtx/cartoons.zip?rlkey=iq2gpqy0q2dvpi8u3htfvyo97&st=byr3wr59&dl=1\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "ph_RDTmXlNcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip cartoons.zip -d cartoons\n",
        "!unzip smoothed_cartoons.zip -d smoothed_cartoons\n",
        "!unzip cocoimg.zip -d cocoimgs -x \"__MACOSX/*\" \"*.DS_Store\""
      ],
      "metadata": {
        "id": "ZcmRwkZg8bKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "# Third-party imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# PyTorch imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CyclicLR\n",
        "from torch.nn import BCELoss\n",
        "from torch import sigmoid\n",
        "\n",
        "# PyTorch vision imports\n",
        "import torchvision.utils as vutils\n",
        "from torchvision import models, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split"
      ],
      "metadata": {
        "id": "eVQHs5AVa1Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by setting up a project directory\n",
        "project_dir = './CartoonGANModel'\n",
        "os.makedirs(project_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "AgSuf_ri85Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define basic parameters\n",
        "image_size = 256  # Size to crop images to\n",
        "batch_size = 16   # Number of images to process in parallel\n",
        "\n",
        "# Define image transformations pipeline\n",
        "transformer = transforms.Compose([\n",
        "    transforms.CenterCrop(image_size),     # Crop the image to a square from the center\n",
        "    transforms.ToTensor()                  # Convert PIL image to tensor and normalize from [0, 255] to [0.0, 1.0]\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  print('Training on GPU')\n",
        "else:\n",
        "  print('Please connect to a GPU runtime')"
      ],
      "metadata": {
        "id": "AUxnrI_d9-5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare cartoon dataset\n",
        "cartoon_dataset = ImageFolder('cartoons/', transformer)\n",
        "len_training_set = math.floor(len(cartoon_dataset) * 0.9)  # Use 90% for training\n",
        "len_valid_set = len(cartoon_dataset) - len_training_set    # Remaining 10% for validation\n",
        "training_set, _ = random_split(cartoon_dataset, (len_training_set, len_valid_set))\n",
        "cartoon_image_dataloader_train = DataLoader(\n",
        "    training_set,\n",
        "    batch_size,\n",
        "    shuffle=True,    # Shuffle data to prevent learning order-dependent patterns\n",
        "    num_workers=0    # Single process data loading\n",
        ")\n",
        "\n",
        "# Load and prepare smoothed cartoon dataset\n",
        "smoothed_cartoon_dataset = ImageFolder('smoothed_cartoons/', transformer)\n",
        "len_training_set = math.floor(len(smoothed_cartoon_dataset) * 0.9)\n",
        "len_valid_set = len(smoothed_cartoon_dataset) - len_training_set\n",
        "training_set, _ = random_split(smoothed_cartoon_dataset, (len_training_set, len_valid_set))\n",
        "smoothed_cartoon_image_dataloader_train = DataLoader(\n",
        "    training_set,\n",
        "    batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Load and prepare photo dataset (from COCO dataset)\n",
        "photo_dataset = ImageFolder('cocoimgs/', transformer)\n",
        "len_training_set = math.floor(len(photo_dataset) * 0.9)\n",
        "len_valid_set = len(photo_dataset) - len_training_set\n",
        "training_set, validation_set = random_split(photo_dataset, (len_training_set, len_valid_set))\n",
        "# We want to split the dataset of real images into testing and training sets\n",
        "photo_dataloader_train = DataLoader(\n",
        "    training_set,\n",
        "    batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "photo_dataloader_valid = DataLoader(\n",
        "    validation_set,\n",
        "    batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "13gC_lgel_o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_sample_image(dataloader):\n",
        "  iterator = iter(dataloader)\n",
        "  sample_batch, _ = next(iterator)\n",
        "  first_sample_image_of_batch = sample_batch[0]\n",
        "  print(first_sample_image_of_batch.size())\n",
        "  print(\"Current range: {} to {}\".format(first_sample_image_of_batch.min(), first_sample_image_of_batch.max()))\n",
        "  plt.imshow(np.transpose(first_sample_image_of_batch.numpy(), (1, 2, 0)))\n",
        "\n",
        "show_sample_image(cartoon_image_dataloader_train)"
      ],
      "metadata": {
        "id": "DV4KUppgdXLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample_image(smoothed_cartoon_image_dataloader_train)"
      ],
      "metadata": {
        "id": "gn7uh_Cyde-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_sample_image(photo_dataloader_train)"
      ],
      "metadata": {
        "id": "6Q08RVCZdkcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's set up our model architecture.\n",
        "\n",
        "We use a pretrained VGG model to extract features from images, and our Generator uses a Residual Block to generate images."
      ],
      "metadata": {
        "id": "OtPb5kF6_F8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load VGG16 model\n",
        "path_to_pretrained_vgg16 = os.path.join(project_dir, 'vgg_torch.pth')\n",
        "\n",
        "try:\n",
        "    # Load the state dictionary instead of the whole model\n",
        "    vgg16 = models.vgg16(weights=None)  # weights=None for no pre-trained weights\n",
        "    vgg16.load_state_dict(torch.load(path_to_pretrained_vgg16))\n",
        "    vgg16 = vgg16.to(device)\n",
        "except FileNotFoundError:\n",
        "    # Download pretrained model and save its state dict\n",
        "    vgg16 = models.vgg16(weights='IMAGENET1K_V1')  # Pretrained weights\n",
        "    torch.save(vgg16.state_dict(), path_to_pretrained_vgg16)\n",
        "    vgg16 = vgg16.to(device)\n",
        "\n",
        "# Extract features up to conv4_3 (since conv4_4 doesn't exist in PyTorch VGG16)\n",
        "feature_extractor = vgg16.features[:23]  # Layer 23 corresponds to conv4_3\n",
        "for param in feature_extractor.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "x2Wz-MTM-bT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv_2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_1 = nn.BatchNorm2d(256)\n",
        "        self.norm_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.norm_2(self.conv_2(F.relu(self.norm_1(self.conv_1(x)))))\n",
        "        return output + x\n",
        "\n",
        "# Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3)\n",
        "        self.norm_1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        # Down-convolution\n",
        "        self.conv_2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv_4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        # Residual blocks\n",
        "        residual_blocks = [ResidualBlock() for _ in range(8)]\n",
        "        self.res = nn.Sequential(*residual_blocks)\n",
        "\n",
        "        # Up-convolution\n",
        "        self.conv_6 = nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_7 = nn.ConvTranspose2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_4 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv_8 = nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.conv_9 = nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_5 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv_10 = nn.Conv2d(64, 3, kernel_size=7, stride=1, padding=3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.norm_1(self.conv_1(x)))\n",
        "\n",
        "        x = F.relu(self.norm_2(self.conv_3(self.conv_2(x))))\n",
        "        x = F.relu(self.norm_3(self.conv_5(self.conv_4(x))))\n",
        "\n",
        "        x = self.res(x)\n",
        "\n",
        "        x = F.relu(self.norm_4(self.conv_7(self.conv_6(x))))\n",
        "        x = F.relu(self.norm_5(self.conv_9(self.conv_8(x))))\n",
        "\n",
        "        x = self.conv_10(x)\n",
        "        x = sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv_1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.conv_2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_1 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv_4 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
        "        self.conv_5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_2 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_6 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
        "        self.norm_3 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv_7 = nn.Conv2d(256, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.conv_1(x), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.norm_1(self.conv_3(F.leaky_relu(self.conv_2(x), negative_slope=0.2))), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.norm_2(self.conv_5(F.leaky_relu(self.conv_4(x), negative_slope=0.2))), negative_slope=0.2)\n",
        "        x = F.leaky_relu(self.norm_3(self.conv_6(x)), negative_slope=0.2)\n",
        "        x = self.conv_7(x)\n",
        "        x = sigmoid(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xIqKrxUQ-Tg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the models\n",
        "G = Generator()\n",
        "D = Discriminator()\n",
        "\n",
        "G.to(device)\n",
        "D.to(device)"
      ],
      "metadata": {
        "id": "pkypvfoh-fJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Discriminator Loss\n",
        "class DiscriminatorLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiscriminatorLoss, self).__init__()\n",
        "        self.bce_loss = BCELoss()\n",
        "\n",
        "    def forward(self, discriminator_output_of_cartoon_input,\n",
        "                discriminator_output_of_cartoon_smoothed_input,\n",
        "                discriminator_output_of_generated_image_input):\n",
        "\n",
        "        return self._adversarial_loss(discriminator_output_of_cartoon_input,\n",
        "                                      discriminator_output_of_cartoon_smoothed_input,\n",
        "                                      discriminator_output_of_generated_image_input)\n",
        "\n",
        "    def _adversarial_loss(self, discriminator_output_of_cartoon_input,\n",
        "                          discriminator_output_of_cartoon_smoothed_input,\n",
        "                          discriminator_output_of_generated_image_input):\n",
        "\n",
        "        # Use zeros and ones with the same shape as the discriminator outputs\n",
        "        zeros = torch.zeros_like(discriminator_output_of_cartoon_input)\n",
        "        ones = torch.ones_like(discriminator_output_of_cartoon_input)\n",
        "\n",
        "        d_loss_cartoon = self.bce_loss(discriminator_output_of_cartoon_input, ones)\n",
        "        d_loss_cartoon_smoothed = self.bce_loss(discriminator_output_of_cartoon_smoothed_input, zeros)\n",
        "        d_loss_generated_input = self.bce_loss(discriminator_output_of_generated_image_input, zeros)\n",
        "\n",
        "        d_loss = d_loss_cartoon + d_loss_cartoon_smoothed + d_loss_generated_input\n",
        "\n",
        "        return d_loss\n",
        "\n",
        "# Define the Generator Loss\n",
        "class GeneratorLoss(nn.Module):\n",
        "    def __init__(self, feature_extractor):\n",
        "        super(GeneratorLoss, self).__init__()\n",
        "        self.w = 0.000005\n",
        "        self.bce_loss = BCELoss()\n",
        "        self.feature_extractor = feature_extractor\n",
        "        for param in self.feature_extractor.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, discriminator_output_of_generated_image_input,\n",
        "                generator_input,\n",
        "                generator_output,\n",
        "                is_init_phase=False):\n",
        "\n",
        "        if is_init_phase:\n",
        "            g_content_loss = self._content_loss(generator_input, generator_output)\n",
        "            g_adversarial_loss = 0.0\n",
        "            g_loss = g_content_loss\n",
        "        else:\n",
        "            g_adversarial_loss = self._adversarial_loss_generator_part_only(discriminator_output_of_generated_image_input)\n",
        "            g_content_loss = self._content_loss(generator_input, generator_output)\n",
        "            g_loss = g_adversarial_loss + self.w * g_content_loss\n",
        "\n",
        "        return g_loss\n",
        "\n",
        "    def _adversarial_loss_generator_part_only(self, discriminator_output_of_generated_image_input):\n",
        "        ones = torch.ones_like(discriminator_output_of_generated_image_input)\n",
        "        return self.bce_loss(discriminator_output_of_generated_image_input, ones)\n",
        "\n",
        "    def _content_loss(self, generator_input, generator_output):\n",
        "        # Compute the content loss between the VGG features of the input and output\n",
        "        vgg_input_features = self.feature_extractor(generator_input)\n",
        "        vgg_output_features = self.feature_extractor(generator_output)\n",
        "        content_loss = F.l1_loss(vgg_output_features, vgg_input_features)\n",
        "        return content_loss\n",
        "\n",
        "# Initialize Loss functions\n",
        "discriminatorLoss = DiscriminatorLoss()\n",
        "generatorLoss = GeneratorLoss(feature_extractor)"
      ],
      "metadata": {
        "id": "XSpvTmgGh8b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's get ready for training!\n",
        "# Set up optimizers\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "beta2 = 0.999\n",
        "\n",
        "d_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "g_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "\n",
        "# Set up directories for saving results and checkpoints\n",
        "os.makedirs(os.path.join(project_dir, 'checkpoints/'), exist_ok=True)\n",
        "intermediate_results_training_path = os.path.join(project_dir, 'intermediate_results/training/')\n",
        "os.makedirs(intermediate_results_training_path, exist_ok=True)\n",
        "\n",
        "# Function to save training results\n",
        "def save_training_result(input, output):\n",
        "    # input/output has batch-size number of images, get first one and detach from tensor\n",
        "    image_input = input[0].detach().cpu().numpy()\n",
        "    image_output = output[0].detach().cpu().numpy()\n",
        "    # Transpose image from torch.Size([3, 256, 256]) to (256, 256, 3)\n",
        "    image_input = np.transpose(image_input, (1, 2, 0))\n",
        "    image_output = np.transpose(image_output, (1, 2, 0))\n",
        "\n",
        "    # Generate filenames as timestamp\n",
        "    filename = str(int(time.time()))\n",
        "    path_input = os.path.join(intermediate_results_training_path, filename + \"_input.jpg\")\n",
        "    path_output = os.path.join(intermediate_results_training_path, filename + \".jpg\")\n",
        "    plt.imsave(path_input, image_input)\n",
        "    plt.imsave(path_output, image_output)\n",
        "\n",
        "# Check for existing checkpoints\n",
        "checkpoint_dir = os.path.join(project_dir, 'checkpoints/')\n",
        "checkpoints = os.listdir(checkpoint_dir)\n",
        "num_epochs = 200 + 10  # Training + initialization phase\n",
        "epochs_already_done = 0\n",
        "best_valid_loss = math.inf\n",
        "losses = []\n",
        "validation_losses = []\n",
        "\n",
        "if len(checkpoints) > 0:\n",
        "    last_checkpoint = sorted(checkpoints)[-1]\n",
        "    checkpoint = torch.load(os.path.join(checkpoint_dir, last_checkpoint), map_location=device)\n",
        "    best_valid_loss = checkpoint['best_valid_loss']\n",
        "    epochs_already_done = checkpoint['last_epoch']\n",
        "    losses = checkpoint['losses']\n",
        "    validation_losses = checkpoint['validation_losses']\n",
        "\n",
        "    D.load_state_dict(checkpoint['d_state_dict'])\n",
        "    G.load_state_dict(checkpoint['g_state_dict'])\n",
        "    d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "    g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "    print('Loaded checkpoint {} with g_valid_loss {}, best_valid_loss {}, {} epochs and total no of losses {}'.format(\n",
        "        last_checkpoint, checkpoint['g_valid_loss'], best_valid_loss, epochs_already_done, len(losses)))"
      ],
      "metadata": {
        "id": "5IOv8I1N-zu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, our training loop!"
      ],
      "metadata": {
        "id": "kYmeNmKw_AcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training function\n",
        "def train(num_epochs, checkpoint_dir, best_valid_loss, epochs_already_done, losses, validation_losses):\n",
        "    init_epochs = 10\n",
        "    print_every = 100\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(epochs_already_done, num_epochs), desc=\"Epochs\"):\n",
        "        D.train()\n",
        "        G.train()\n",
        "        is_init_phase = epoch < init_epochs\n",
        "\n",
        "        # Training batches\n",
        "        for index, ((photo_images, _), (smoothed_cartoon_images, _), (cartoon_images, _)) in enumerate(\n",
        "                tqdm(zip(photo_dataloader_train, smoothed_cartoon_image_dataloader_train, cartoon_image_dataloader_train),\n",
        "                     desc=\"Batches\", leave=False)):\n",
        "\n",
        "            batch_size = photo_images.size(0)\n",
        "            photo_images = photo_images.to(device)\n",
        "            smoothed_cartoon_images = smoothed_cartoon_images.to(device)\n",
        "            cartoon_images = cartoon_images.to(device)\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            d_of_cartoon_input = D(cartoon_images)\n",
        "            d_of_cartoon_smoothed_input = D(smoothed_cartoon_images)\n",
        "            generated_images = G(photo_images)\n",
        "            d_of_generated_image_input = D(generated_images.detach())\n",
        "\n",
        "            d_loss = discriminatorLoss(d_of_cartoon_input,\n",
        "                                       d_of_cartoon_smoothed_input,\n",
        "                                       d_of_generated_image_input)\n",
        "\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train the generator\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            generated_images = G(photo_images)\n",
        "            d_of_generated_image_input = D(generated_images)\n",
        "\n",
        "            g_loss = generatorLoss(d_of_generated_image_input,\n",
        "                                   photo_images,\n",
        "                                   generated_images,\n",
        "                                   is_init_phase=is_init_phase)\n",
        "\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            if (index % print_every) == 0:\n",
        "                losses.append((d_loss.item(), g_loss.item()))\n",
        "                now = time.time()\n",
        "                current_run_time = now - start_time\n",
        "                start_time = now\n",
        "                print(\"Epoch {}/{} | d_loss {:6.4f} | g_loss {:6.4f} | time {:2.0f}s | total no. of losses {}\".format(\n",
        "                    epoch+1, num_epochs, d_loss.item(), g_loss.item(), current_run_time, len(losses)))\n",
        "\n",
        "        save_training_result(photo_images, generated_images)\n",
        "\n",
        "        # Validation\n",
        "        D.eval()\n",
        "        G.eval()\n",
        "        with torch.no_grad():\n",
        "            total_g_valid_loss = 0.0\n",
        "            num_batches = 0\n",
        "            for batch_index, (photo_images, _) in enumerate(tqdm(photo_dataloader_valid, desc=\"Validation Batches\", leave=False)):\n",
        "                photo_images = photo_images.to(device)\n",
        "                generated_images = G(photo_images)\n",
        "                d_of_generated_image_input = D(generated_images)\n",
        "                g_valid_loss = generatorLoss(d_of_generated_image_input,\n",
        "                                             photo_images,\n",
        "                                             generated_images,\n",
        "                                             is_init_phase=is_init_phase)\n",
        "                total_g_valid_loss += g_valid_loss.item()\n",
        "                num_batches += 1\n",
        "\n",
        "            avg_g_valid_loss = total_g_valid_loss / num_batches\n",
        "            validation_losses.append(avg_g_valid_loss)\n",
        "            print(\"Epoch {}/{} | validation loss {:6.4f}\".format(epoch+1, num_epochs, avg_g_valid_loss))\n",
        "\n",
        "        # Checkpointing\n",
        "        if avg_g_valid_loss < best_valid_loss:\n",
        "            print(\"Generator loss improved from {} to {}\".format(best_valid_loss, avg_g_valid_loss))\n",
        "            best_valid_loss = avg_g_valid_loss\n",
        "\n",
        "        checkpoint = {'g_valid_loss': avg_g_valid_loss,\n",
        "                      'best_valid_loss': best_valid_loss,\n",
        "                      'losses': losses,\n",
        "                      'validation_losses': validation_losses,\n",
        "                      'last_epoch': epoch+1,\n",
        "                      'd_state_dict': D.state_dict(),\n",
        "                      'g_state_dict': G.state_dict(),\n",
        "                      'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "                      'g_optimizer_state_dict': g_optimizer.state_dict()\n",
        "                     }\n",
        "        print(\"Save checkpoint for validation loss of {}\".format(avg_g_valid_loss))\n",
        "        torch.save(checkpoint, os.path.join(checkpoint_dir, 'checkpoint_epoch_{:03d}.pth'.format(epoch+1)))\n",
        "        if best_valid_loss == avg_g_valid_loss:\n",
        "            print(\"Overwrite best checkpoint\")\n",
        "            torch.save(checkpoint, os.path.join(checkpoint_dir, 'best_checkpoint.pth'))\n",
        "\n",
        "    return losses, validation_losses"
      ],
      "metadata": {
        "id": "mYMyejNv-pLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses, validation_losses = train(num_epochs, checkpoint_dir, best_valid_loss, epochs_already_done, losses, validation_losses)"
      ],
      "metadata": {
        "id": "aMgP-NGb_CMQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now to run inference with our model!\n",
        "\n",
        "Since we have limited time, you probably won't have a very well-trained model. For convenience, you can use a trained model here."
      ],
      "metadata": {
        "id": "oMCk2lbKxHj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Download the pretrained model weights\n",
        "\"\"\"\n",
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "file_id = '1wcCs3Q7b6fhKqa1ET3oWGdE-CkO-Bd67'\n",
        "direct_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = 'generator.zip'  # Use .zip extension\n",
        "gdown.download(direct_url, output, quiet=False)\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile('generator.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    print(\"File extracted successfully!\")\n",
        "    os.remove('generator.zip')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "id": "vh-wv8P0yZhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load('./generator_latest.pth', map_location=device, weights_only=True)\n",
        "\n",
        "if checkpoint is not None:\n",
        "    # Initialize your model and load the state dict\n",
        "    G_inference = Generator()  # Make sure Generator is defined\n",
        "    G_inference.load_state_dict(checkpoint['g_state_dict'])"
      ],
      "metadata": {
        "id": "KTBLqpK8xRL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def cartoonize_and_display(image_path, model_path='./generator_latest.pth', device='cpu'):\n",
        "    \"\"\"\n",
        "    Load an image, cartoonize it, and display both original and cartoonized versions side by side\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to input image\n",
        "        model_path (str): Path to pretrained generator weights\n",
        "        device (str): 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    # Load and prepare image\n",
        "    transformer = transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # Load and transform original image\n",
        "    original_image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transformer(original_image).unsqueeze(0).to(device)\n",
        "\n",
        "    # Load model\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    G = Generator()\n",
        "    G.load_state_dict(checkpoint['g_state_dict'])\n",
        "    G.to(device)\n",
        "    G.eval()\n",
        "\n",
        "    # Run inference\n",
        "    # We \"freeze\" the model as it is so that PyTorch doesn't update the weights\n",
        "    with torch.no_grad():\n",
        "        output = G(image_tensor)\n",
        "\n",
        "    # Convert output tensor to an image by moving it from our GPU to CPU and then converting it to a Numpy array\n",
        "    cartoonized = output[0].cpu().detach().numpy()\n",
        "    cartoonized = np.transpose(cartoonized, (1, 2, 0))\n",
        "    cartoonized = np.clip(cartoonized, 0, 1)  # Ensure values are in [0,1]\n",
        "\n",
        "    # Get the transformed original image for comparison\n",
        "    original = image_tensor[0].cpu().detach().numpy()\n",
        "    original = np.transpose(original, (1, 2, 0))\n",
        "\n",
        "    # Create side by side plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(original)\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cartoonized)\n",
        "    plt.title('Cartoonized Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "cartoonize_and_display(\n",
        "    image_path='./images/cow.jpg'\n",
        ")"
      ],
      "metadata": {
        "id": "tyxstiVy4duE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now let's cartoonize images on our webcam!\n",
        "\n"
      ],
      "metadata": {
        "id": "go26ZEeA5oT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def take_photo():\n",
        "    \"\"\"Take a photo using the webcam in Google Colab.\"\"\"\n",
        "    js_code = \"\"\"\n",
        "    async function takePhoto() {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        const video = document.createElement('video');\n",
        "        const canvas = document.createElement('canvas');\n",
        "\n",
        "        capture.textContent = 'Capture';\n",
        "        div.appendChild(video);\n",
        "        div.appendChild(capture);\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "\n",
        "        // Get video stream\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Set up canvas\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "\n",
        "        // Take photo when button clicked\n",
        "        return new Promise((resolve) => {\n",
        "            capture.onclick = () => {\n",
        "                canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "                stream.getVideoTracks()[0].stop();\n",
        "                div.remove();\n",
        "                resolve(canvas.toDataURL('image/jpeg', 0.8));\n",
        "            };\n",
        "        });\n",
        "    }\n",
        "    takePhoto();\n",
        "    \"\"\"\n",
        "\n",
        "    # Display JavaScript code and execute the function\n",
        "    display(Javascript(js_code))\n",
        "    photo_data = eval_js(js_code)\n",
        "    binary = b64decode(photo_data.split(',')[1])\n",
        "    return binary\n",
        "\n",
        "def webcam_to_cartoon(model_path='./generator_latest.pth', device='cpu'):\n",
        "    \"\"\"\n",
        "    Take a photo from webcam and cartoonize it\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to pretrained generator weights\n",
        "        device (str): 'cuda' or 'cpu'\n",
        "    \"\"\"\n",
        "    print(\"Click 'Capture' to take a photo when your webcam opens\")\n",
        "\n",
        "    try:\n",
        "        # Take photo\n",
        "        image_binary = take_photo()\n",
        "\n",
        "        # Save temporary file\n",
        "        temp_filename = 'temp_webcam.jpg'\n",
        "        with open(temp_filename, 'wb') as f:\n",
        "            f.write(image_binary)\n",
        "\n",
        "        # Cartoonize and display (implement your cartoonize_and_display function)\n",
        "        cartoonized = cartoonize_and_display(temp_filename, model_path, device)\n",
        "\n",
        "        # Clean up\n",
        "        import os\n",
        "        os.remove(temp_filename)\n",
        "\n",
        "        return cartoonized\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "        print(\"Make sure you've allowed camera access in your browser.\")\n",
        "\n",
        "# Inform the user about camera access\n",
        "HTML(\"\"\"\n",
        "<div class=\"alert alert-info\">\n",
        "    <h3>Important:</h3>\n",
        "    <p>If this is your first time using the webcam in Colab, you'll need to allow camera access.</p>\n",
        "    <p>Look for the camera permission popup in your browser.</p>\n",
        "</div>\n",
        "\"\"\")\n",
        "\n",
        "# To use:\n",
        "webcam_to_cartoon()"
      ],
      "metadata": {
        "id": "HWAV4JTT5t-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}